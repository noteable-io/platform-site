---
slug: notebook-tools-for-llms
title: "Notebooks: A Tool for LLMs"
authors: [kyle]
tags: [chatgpt, ipython, notebooks, noteable]
---

import { OutputBlock } from "@site/src/components/cell";

import {
  Chat,
  UserMessage,
  AssistantMessage,
  ChatFunctionCall,
} from "@site/src/components/chat";

## Introduction

```python cell count=1
import openai
import noteable
```

The Noteable Plugin for ChatGPT exposes rich computational notebooks to GPT Models, allowing you to experiment with data in new ways and lean on ChatGPT as an automated agent.

Notebooks are a pivotal tool for data scientists, researchers, and engineers. Now, they're available for everyone. Notebooks allow us to communicate ideas, explore data, and share results. Notebooks are a tool for thinking. Most people use them to explore data. They're also a great little laboratory for experimenting with APIs.

Artificial Intelligence companies like OpenAI clearly included notebooks as part of the corpus of training data because their **Large Language Models know notebooks**. ChatGPT can create well documented notebooks with code and prose. It has shown me whole new ways of using libraries and analyzing data that previously were out of reach or just unknown to me. Anyone who knows notebooks and interactive computing in general, knows what this means. It's a big deal.

<Chat>
<UserMessage avatar="https://github.com/rgbkrk.png">

Please create an ETL notebook for the [IMDb dataset](https://datasets.imdbws.com/)

</UserMessage>

<AssistantMessage avatar="">

<ChatFunctionCall>

```json chatcall request plugin=Noteable
{
  "notebook_name": "IMDb_ETL.ipynb"
}
```

```json chatcall response plugin=Noteable
{
  "project_id": "3b179d03-9efb-4f6c-ad7a-fa0a478d45e4",
  "file_id": "d3d944d0-5a18-48b3-bf3e-fdb59d8a5dea",
  "notebook_url": "https://app.noteable.io/f/d3d944d0-5a18-48b3-bf3e-fdb59d8a5dea/IMDb_ETL.ipynb",
  "assistant_hint": "The user can now follow along with your cell work at the notebook link."
}
```

</ChatFunctionCall>

I have created a new notebook named [IMDb_ETL.ipynb](https://app.noteable.io/f/d3d944d0-5a18-48b3-bf3e-fdb59d8a5dea/IMDb_ETL.ipynb) for the IMDb dataset ETL process. You can access it directly. Now, let's start with the ETL process.

</AssistantMessage>
</Chat>

Early experiments with integrating LLMs into notebooks were very promising. Open source projects have shown the potential of integrating IPython and Jupyter with Large Language Models ([Jupyter AI](https://github.com/jupyterlab/jupyter-ai), [genai](https://github.com/noteable-io/genai)). These projects allow for deeper exception handling, inline assistance, and even code generation. It's fantastic. Due to technical limitations in open source Jupyter though, there's no realtime server side access to the notebook document model.

What we needed was a way for the model to be able to create and edit cells, start new notebooks, and debug its own code. Everything that would be part of the workflow for a data scientist, data engineer, or anyone else looking to learn in a notebook.

## Noteable's Headless Execution

Building on top of our foundations for [realtime collaboration and scheduled notebooks](https://github.com/noteable-io/papermill-origami), we began work on a plugin that would allow the model to do just that. It started with creating notebooks

```python cell count=2
conversation_notebook = await noteable.create_notebook(
    project_id=default_project,
    notebook_name="IMDb_ETL.ipynb"
)
```

and creating cells

```python cell count=3
source="""# IMDb Dataset ETL
This notebook will perform the Extract, Transform, Load (ETL) process
for the IMDb dataset available at https://datasets.imdbws.com/. The
dataset includes several files, we will focus on the following:

- `title.basics.tsv.gz`: Contains information for titles.
- `title.ratings.tsv.gz`: Contains the IMDb rating and votes information for titles.

Let's start by downloading the data."""

await noteable.create_cell(
  file_id=conversation_notebook,
  cell_type="markdown",
  source=source
)
```

What delighted me the most out of all of this is that ChatGPT knew how to create literate computational notebooks right out of the gate.

```python cell count=3
source = """
!mkdir -p /tmp/imdb_data
!wget -nc -P /tmp/imdb_data/ https://datasets.imdbws.com/title.basics.tsv.gz
!wget -nc -P /tmp/imdb_data/ https://datasets.imdbws.com/title.ratings.tsv.gz
""".strip()

cell = await conversation_notebook.create_cell(source)
outputs = await cell.run()
```

From here, ChatGPT is able to analyze data and navigate its way through errors. It's ability to debug is incredible. It sometimes gets stuck, but who among us doesn't have that problem?

## Contrasting with OpenAI's Code Interpreter

There isn't just one way to code. There are also many ways to create data products. The difference between these tools is in the interface and the environment provided. The Large Language Model underneath is roughly the same.

Some of the exact same reasons you see magic ðŸª„ when using Noteable is the same reason you see magic ðŸª„ using OpenAI's Code Interpreter. It can reason, debug, run code, analyze data, etc. What's even better is that Code Interpreter is fast to spin up and work with. They've done great prompting and RLHF behind the scenes to make it a great experience.

One of OpenAI Code Interpreter's greatest strengths is also a core limitation. The very nature of code interpreter is ephemeral. The greatest thing about ephemeral locked down compute is that you can aggressively cache. I know this from [creating tmpnb back in 2014](https://speakerdeck.com/rgbkrk/tmpnb-at-pytexas?slide=10), which we used to [provide notebooks to readers of an IPython article in the Nature Journal](https://www.nature.com/articles/515151a). That also formed the foundations of JupyterHub's Dockerspawner and greatly [informed how we needed to optimize the Jupyter Notebook](https://lambdaops.com/ops-lessons-and-instant-temporary-ipython-jupyter-notebooks/). If you're using JupyterHub now, you're benefiting from some of the ops lessons learned way back then.

Not long after we had this deployed for everyone, we started finding out that people wanted to use our temporary notebooks for real analysis work. We fundamentally disrupted the problem of _access_ and people wanted _more_.

## Providing Deeper Access with Notebooks

Eventually, you'll want more levers. I know I do. I need access to data files, environment variables/secrets, and databases. Exposing notebooks through ChatGPT provides a window into this workspace:

- The result of computation with the model is stored in a data driven document, a notebook
- Notebooks are expressive, reproducible communication tools that you can collaborate on with other people
- A flexible hosted environment expands the possibilities of analysis tasks available

As a ChatGPT Plugin, we're limited in the way in which we can expose notebook compute. In order to create a plugin we had to wrap our realtime API with a REST API to allow the GPT models to have access to notebook operations like creating, running, and updating cells. Given that we created our notebook platform for analysis work, we aimed for reproducibility as one of our design constraints. That means that project files for users are mounted with their kernels/notebooks and they have access to their data connections and secrets. Compared to code interpreter, it's not as fast to just have the LLM write code. It gives you persistence though. We also don't control the model or the temperature.

Some of the limitations here are much better handled with direct usage of our realtime service along with OpenAI's chat functions, where there's a lot more control and even speed, context, and customization. OpenAI has created really great APIs and standards for working with their Large Language Models, relying on OpenAPI and JSON Schemas extensively. There are three key components of their APIs that I plan to build on top of with notebooks:

- ChatGPT Plugins
- Chat Completions API
- Function Calling
